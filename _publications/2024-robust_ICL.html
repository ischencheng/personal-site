---
layout: publication
year: 2024
title: "A Closer Look at the Robustness of In-Context Learning with Noisy Labels"
authors:
  - Chen Cheng*
  - Xinzhi Yu*
  - Haodong Wen*
  - Jingsong Sun
  - Guanzhang Yue
  - Yihao Zhang
  - Zeming Wei
highlight: true
type:
  - Conference
  - Workshop
venue: Reliable and Responsible Foundation Models Workshop at ICLR
venue_location: Vienna, Austria
venue_tags:
  - ICLR 
tags:
  - LLM
code: https://github.com/InezYu0928/in-context-learning
arxiv: "Upcoming"
pdf: /papers/ICLRW_2024_Noisy_Label_ICL.pdf
---

Recently, the mysterious In-Context Learning (ICL) prowess exhibited by Transformer architectures, especially in large language models (LLMs), has sparked significant research interest. However, the resilience of Transformers' in-context learning capabilities in the presence of noisy samples, prevalent in both training corpora and prompt demonstrations, remains underexplored. In this paper, inspired by prior research that study ICL ability using simple function classes, we delve deeper into this issue by investigating the robustness of ICL Transformers against noisy labels. Specifically, we conduct a thorough assessment demonstrating that Transformers exhibit notable resilience against diverse types of noise in demonstration labels, surpassing prior simplistic observations. Furthermore, we explore whether introducing noise into the training set, akin to a form of data augmentation, enhances such robustness during inference. Our comprehensive findings provide valuable insights into the resilience of Transformer models against label noise, thereby laying a foundational framework for further advancements in this domain.